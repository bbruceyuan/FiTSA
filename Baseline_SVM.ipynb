{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import numpy as np\n",
    "\n",
    "train=pd.read_csv('data/train.tsv', sep='\\t',header=0)\n",
    "test=pd.read_csv('data/test.tsv', sep='\\t',header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train[['sentence']]\n",
    "y_train = train.label\n",
    "X_test = test[['sentence']]\n",
    "y_test = test.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "X_train['cutted_comment'] = X_train.sentence.apply(lambda x: \" \".join(jieba.cut(x)))\n",
    "X_test['cutted_comment'] = X_test.sentence.apply(lambda x: \" \".join(jieba.cut(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    派思 股份 与 自贡 华燃 之间 的 交易 ， 便是 根据 辽宁 众华 出具 的 上述 评估...\n",
       "1    市场 认为 ， 从 财务数据 来看 ， 宣亚 的 收购 是 划算 的 。 宣亚 2016 年...\n",
       "2    对于 业绩 下滑 ， 智慧 松德 给出 的 解释 是 ， 承担 公司 主要 智能 装备 业务...\n",
       "3    此前 ， 不少 投资者 在 股 吧 中 讨论 认为 ， 甲基 卡锂 矿复产 将 对 融捷 股...\n",
       "4    问及 信 达 生物 此前 主动 撤回 信迪 单抗 注射液 上市 申请 是否 影响 其 港股 ...\n",
       "Name: cutted_comment, dtype: object"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.cutted_comment[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_custom_stopwords(stop_words_file):\n",
    "    with open(stop_words_file) as f:\n",
    "        stopwords = f.read()\n",
    "    stopwords_list = stopwords.split('\\n')\n",
    "    custom_stopwords_list = [i for i in stopwords_list]\n",
    "    return custom_stopwords_list\n",
    "stop_words_file = \"stopwordsHIT.txt\"\n",
    "stopwords = get_custom_stopwords(stop_words_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from pprint import pprint\n",
    "pipe = Pipeline([\n",
    "    ('vect', CountVectorizer(token_pattern=u'(?u)\\\\b[^\\\\d\\\\W]\\\\w+\\\\b',\n",
    "                             max_df = 0.5,\n",
    "                             min_df=3,\n",
    "                             max_features = None,\n",
    "                             ngram_range = (1, 1),\n",
    "                             stop_words = frozenset(stopwords))),\n",
    "    ('tfidf', TfidfTransformer(norm = 'l2', use_idf = 'True')),\n",
    "    ('svm', SVC()),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = [{'svm__kernel': ['rbf'], 'svm__gamma': [1e-3, 1e-4],'svm__C': [1, 10, 100, 1000]},\n",
    "              {'svm__kernel': ['linear'], 'svm__C': [1, 3,5,7,10]}]\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "grid_search = GridSearchCV(pipe, parameters, scoring='f1_macro', cv=5, n_jobs=-1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 13 candidates, totalling 65 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:   36.8s\n",
      "[Parallel(n_jobs=-1)]: Done  65 out of  65 | elapsed:  1.1min finished\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['exp', 'lex', '①①', '①②', '①③', '①④', '①⑤', '①⑥', '①⑦', '①⑧', '①⑨', '①ａ', '①ｂ', '①ｃ', '①ｄ', '①ｅ', '①ｆ', '①ｇ', '①ｈ', '①ｉ', '①ｏ', '②①', '②②', '②③', '②④', '②⑤', '②⑥', '②⑦', '②⑧', '②⑩', '②ａ', '②ｂ', '②ｄ', '②ｅ', '②ｆ', '②ｇ', '②ｈ', '②ｉ', '②ｊ', '③①', '③⑩', '③ａ', '③ｂ', '③ｃ', '③ｄ', '③ｅ', '③ｆ', '③ｇ', '③ｈ', '④ａ', '④ｂ', '④ｃ', '④ｄ', '④ｅ', '⑤ａ', '⑤ｂ', '⑤ｄ', '⑤ｅ', '⑤ｆ', 'ｌｉ', 'ｚｘｆｉｔｌ'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "             estimator=Pipeline(memory=None,\n",
       "                                steps=[('vect',\n",
       "                                        CountVectorizer(analyzer='word',\n",
       "                                                        binary=False,\n",
       "                                                        decode_error='strict',\n",
       "                                                        dtype=<class 'numpy.int64'>,\n",
       "                                                        encoding='utf-8',\n",
       "                                                        input='content',\n",
       "                                                        lowercase=True,\n",
       "                                                        max_df=0.5,\n",
       "                                                        max_features=None,\n",
       "                                                        min_df=3,\n",
       "                                                        ngram_range=(1, 1),\n",
       "                                                        preprocessor=None,\n",
       "                                                        stop_words=frozenset({'\"',\n",
       "                                                                              '#',\n",
       "                                                                              '&',\n",
       "                                                                              \"'\",...\n",
       "                                            kernel='rbf', max_iter=-1,\n",
       "                                            probability=False,\n",
       "                                            random_state=None, shrinking=True,\n",
       "                                            tol=0.001, verbose=False))],\n",
       "                                verbose=False),\n",
       "             iid='warn', n_jobs=-1,\n",
       "             param_grid=[{'svm__C': [1, 10, 100, 1000],\n",
       "                          'svm__gamma': [0.001, 0.0001],\n",
       "                          'svm__kernel': ['rbf']},\n",
       "                         {'svm__C': [1, 3, 5, 7, 10],\n",
       "                          'svm__kernel': ['linear']}],\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring='f1_macro', verbose=1)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.fit(X_train.cutted_comment, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.677\n",
      "Best parameters set:\n",
      "\tsvm__C: 1\n",
      "\tsvm__gamma: 'auto_deprecated'\n",
      "\tsvm__kernel: 'linear'\n",
      "\tsvm__C: 1\n",
      "\tsvm__kernel: 'linear'\n"
     ]
    }
   ],
   "source": [
    "print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "for param in parameters:\n",
    "    for param_name in sorted(param.keys()):\n",
    "        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
       "    kernel='linear', max_iter=-1, probability=False, random_state=None,\n",
       "    shrinking=True, tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_parameters['svm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.69      0.67       725\n",
      "           1       0.76      0.57      0.65       368\n",
      "           2       0.76      0.81      0.79       907\n",
      "\n",
      "    accuracy                           0.72      2000\n",
      "   macro avg       0.73      0.69      0.70      2000\n",
      "weighted avg       0.72      0.72      0.72      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pre = grid_search.predict(X_test.cutted_comment)\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pre))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.85      0.85      2157\n",
      "           1       0.91      0.79      0.85      1072\n",
      "           2       0.89      0.93      0.91      2771\n",
      "\n",
      "    accuracy                           0.88      6000\n",
      "   macro avg       0.88      0.86      0.87      6000\n",
      "weighted avg       0.88      0.88      0.88      6000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pre = grid_search.predict(X_train.cutted_comment)\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_train, y_pre))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
