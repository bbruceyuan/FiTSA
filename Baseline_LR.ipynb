{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import numpy as np\n",
    "\n",
    "train=pd.read_csv('data/train.tsv', sep='\\t',header=0)\n",
    "test=pd.read_csv('data/test.tsv', sep='\\t',header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train[['sentence']]\n",
    "y_train = train.label\n",
    "X_test = test[['sentence']]\n",
    "y_test = test.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache C:\\Users\\ALICKZ~1\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.615 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "X_train['cutted_comment'] = X_train.sentence.apply(lambda x: \" \".join(jieba.cut(x)))\n",
    "X_test['cutted_comment'] = X_test.sentence.apply(lambda x: \" \".join(jieba.cut(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    派思 股份 与 自贡 华燃 之间 的 交易 ， 便是 根据 辽宁 众华 出具 的 上述 评估...\n",
       "1    市场 认为 ， 从 财务数据 来看 ， 宣亚 的 收购 是 划算 的 。 宣亚 2016 年...\n",
       "2    对于 业绩 下滑 ， 智慧 松德 给出 的 解释 是 ， 承担 公司 主要 智能 装备 业务...\n",
       "3    此前 ， 不少 投资者 在 股 吧 中 讨论 认为 ， 甲基 卡锂 矿复产 将 对 融捷 股...\n",
       "4    问及 信 达 生物 此前 主动 撤回 信迪 单抗 注射液 上市 申请 是否 影响 其 港股 ...\n",
       "Name: cutted_comment, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.cutted_comment[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_custom_stopwords(stop_words_file):\n",
    "    with open(stop_words_file) as f:\n",
    "        stopwords = f.read()\n",
    "    stopwords_list = stopwords.split('\\n')\n",
    "    custom_stopwords_list = [i for i in stopwords_list]\n",
    "    return custom_stopwords_list\n",
    "stop_words_file = \"stopwordsHIT.txt\"\n",
    "stopwords = get_custom_stopwords(stop_words_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from pprint import pprint\n",
    "pipe = Pipeline([\n",
    "    ('vect', CountVectorizer(token_pattern=u'(?u)\\\\b[^\\\\d\\\\W]\\\\w+\\\\b', \n",
    "                       stop_words=frozenset(stopwords))),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('lr', LogisticRegression()),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'vect__max_df': [0.5,0.8, 1.0],\n",
    "    'vect__max_features': (None, 3, 5),\n",
    "    'vect__ngram_range': ((1, 1), (1, 2)),  # unigrams or bigrams\n",
    "    'tfidf__use_idf': (True, False),\n",
    "    'tfidf__norm': ('l1', 'l2'),\n",
    "}\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "grid_search = GridSearchCV(pipe, parameters, scoring='f1_macro', cv=5, n_jobs=-1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:    4.6s\n",
      "[Parallel(n_jobs=-1)]: Done 176 tasks      | elapsed:   16.6s\n",
      "[Parallel(n_jobs=-1)]: Done 360 out of 360 | elapsed:   32.6s finished\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['exp', 'lex', '①①', '①②', '①③', '①④', '①⑤', '①⑥', '①⑦', '①⑧', '①⑨', '①ａ', '①ｂ', '①ｃ', '①ｄ', '①ｅ', '①ｆ', '①ｇ', '①ｈ', '①ｉ', '①ｏ', '②①', '②②', '②③', '②④', '②⑤', '②⑥', '②⑦', '②⑧', '②⑩', '②ａ', '②ｂ', '②ｄ', '②ｅ', '②ｆ', '②ｇ', '②ｈ', '②ｉ', '②ｊ', '③①', '③⑩', '③ａ', '③ｂ', '③ｃ', '③ｄ', '③ｅ', '③ｆ', '③ｇ', '③ｈ', '④ａ', '④ｂ', '④ｃ', '④ｄ', '④ｅ', '⑤ａ', '⑤ｂ', '⑤ｄ', '⑤ｅ', '⑤ｆ', 'ｌｉ', 'ｚｘｆｉｔｌ'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "             estimator=Pipeline(memory=None,\n",
       "                                steps=[('vect',\n",
       "                                        CountVectorizer(analyzer='word',\n",
       "                                                        binary=False,\n",
       "                                                        decode_error='strict',\n",
       "                                                        dtype=<class 'numpy.int64'>,\n",
       "                                                        encoding='utf-8',\n",
       "                                                        input='content',\n",
       "                                                        lowercase=True,\n",
       "                                                        max_df=1.0,\n",
       "                                                        max_features=None,\n",
       "                                                        min_df=1,\n",
       "                                                        ngram_range=(1, 1),\n",
       "                                                        preprocessor=None,\n",
       "                                                        stop_words=frozenset({'\"',\n",
       "                                                                              '#',\n",
       "                                                                              '&',\n",
       "                                                                              \"'\",...\n",
       "                                                           random_state=None,\n",
       "                                                           solver='warn',\n",
       "                                                           tol=0.0001,\n",
       "                                                           verbose=0,\n",
       "                                                           warm_start=False))],\n",
       "                                verbose=False),\n",
       "             iid='warn', n_jobs=-1,\n",
       "             param_grid={'tfidf__norm': ('l1', 'l2'),\n",
       "                         'tfidf__use_idf': (True, False),\n",
       "                         'vect__max_df': [0.5, 0.8, 1.0],\n",
       "                         'vect__max_features': (None, 3, 5),\n",
       "                         'vect__ngram_range': ((1, 1), (1, 2))},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring='f1_macro', verbose=1)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.fit(X_train.cutted_comment, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.660\n",
      "Best parameters set:\n",
      "\ttfidf__norm: 'l2'\n",
      "\ttfidf__use_idf: True\n",
      "\tvect__max_df: 0.5\n",
      "\tvect__max_features: None\n",
      "\tvect__ngram_range: (1, 1)\n"
     ]
    }
   ],
   "source": [
    "print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.67      0.68       725\n",
      "           1       0.77      0.46      0.58       368\n",
      "           2       0.74      0.87      0.80       907\n",
      "\n",
      "    accuracy                           0.72      2000\n",
      "   macro avg       0.73      0.67      0.68      2000\n",
      "weighted avg       0.72      0.72      0.71      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pre = grid_search.predict(X_test.cutted_comment)\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pre))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.80      0.81      2157\n",
      "           1       0.87      0.61      0.71      1072\n",
      "           2       0.83      0.94      0.88      2771\n",
      "\n",
      "    accuracy                           0.83      6000\n",
      "   macro avg       0.84      0.78      0.80      6000\n",
      "weighted avg       0.83      0.83      0.82      6000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pre = grid_search.predict(X_train.cutted_comment)\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_train, y_pre))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
